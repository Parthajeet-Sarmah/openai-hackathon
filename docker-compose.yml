services:
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models
    command: [
      "--model", "/models/openai_gpt-oss-20b-MXFP4.gguf",
      "--host", "0.0.0.0",
      "--port", "8000",
      "--n_gpu_layers", "0", #Adjust based on GPU VRAM, I only have 4GB on my laptop :(
      "--reasoning-format", "none",
      "--chat-template-kwargs", '{"reasoning_effort": "disabled"}'
    ]
    networks:
      - activity-monitor
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  activity-monitor: